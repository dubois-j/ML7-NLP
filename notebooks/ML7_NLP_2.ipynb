{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ccd6d3",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks : Long Short Term Memory networks (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741fef6c",
   "metadata": {},
   "source": [
    "We have already covered feed-forward neural networks during the computer vision and the recommended system module. For natural language processing, one type of popular deep learning architecture is called Reccurent Neural Networks (RNNs). RNNs differ from feed-forward networks in the sense that some of their inner layers are recursively updated while iterating over the sequence of words given in input. We are going to use one specific RNN architecture called Long-Short Term Memory networks (LSTMs), which have been especially successful in various NLP tasks, including automatic translation, question answering, ... and text classification, our case study in this module.\n",
    "\n",
    "Learn more about how RNNs and LSTMs encode texts as vectors first:\n",
    "\n",
    "https://medium.com/towards-data-science/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
    "\n",
    "If you want to understand in depth how one LSTM cell is working, you can go through these two articles:\n",
    "\n",
    "https://medium.com/towards-data-science/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291f1f6",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '../data/imdb_clean/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b8ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_deep_clean = pd.read_csv(outdir + 'train.csv')\n",
    "valid_deep_clean = pd.read_csv(outdir + 'valid.csv')\n",
    "test_deep_clean = pd.read_csv(outdir + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3d33d",
   "metadata": {},
   "source": [
    "## Implementing LSTMs with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some lines that allow for faster training with this version of tensorflow for these models\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d51aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import TextVectorization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98fd0b0",
   "metadata": {},
   "source": [
    "We will use Keras to implement an LSTM network. First we need to encode all the reviews as a list of indexes, where each word is replaced by its embedding index using keras `TextVectorization` object. To make all training reviews the same size, the `TextVectorization` will add a special token < pad > (encoded as 0) as many time as necessary to each text to match the length of the longest text. This token will be ignored by the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc3e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 10000 \n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=max_vocab_size, output_mode='int',split='whitespace')\n",
    "vectorizer.adapt(pd.concat([train_deep_clean,valid_deep_clean]).review)\n",
    "\n",
    "# This encodes our sentence as a sequence of integer\n",
    "# each integer being the index of each word in the vocabulary\n",
    "# Zeros are added at the end so that each text has the same length\n",
    "train_seqs = vectorizer(train_deep_clean.review)\n",
    "valid_seqs = vectorizer(valid_deep_clean.review)\n",
    "test_seqs = vectorizer(test_deep_clean.review)\n",
    "\n",
    "max_seq_length = train_seqs.shape[1]\n",
    "\n",
    "#However due to a bug in Keras 3, we need to put the zeros at the beginning, not at the end:\n",
    "def repad_left(x, maxlen):       \n",
    "    rows = []\n",
    "    x_arr = np.array(x)\n",
    "    x_arr = np.concatenate((x_arr, np.zeros((x_arr.shape[0],1))), axis=1)\n",
    "    for row in x_arr:\n",
    "        rows.append(row[:row.argmin()])\n",
    "    ledt_padded_x = keras.utils.pad_sequences(rows, maxlen=maxlen)\n",
    "    return ledt_padded_x\n",
    "        \n",
    "\n",
    "X_train = repad_left(train_seqs, max_seq_length)\n",
    "X_valid = repad_left(valid_seqs, max_seq_length)\n",
    "X_test = repad_left(test_seqs, max_seq_length)\n",
    "\n",
    "#Finally we encode the ys :\n",
    "y_train = pd.get_dummies(train_deep_clean.sentiment).values[:,1]\n",
    "y_valid = pd.get_dummies(valid_deep_clean.sentiment).values[:,1]\n",
    "y_test = pd.get_dummies(test_deep_clean.sentiment).values[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04acb2d",
   "metadata": {},
   "source": [
    "Now fill the following function to implement a simple LSTM model : one embedding layer, one LSTM layer, and a final dense layer that yields a single score with a sigmoid activation function. Use Keras' Sequential API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58444f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model(vocab_size, embedding_dim, seq_length, lstm_out_dim):\n",
    "    #TOFILL\n",
    "\n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='SGD',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200  #Bigger than embedding dim, as it combines all the words of each review\n",
    "\n",
    "model = get_lstm_model(max_vocab_size, embedding_dim, max_seq_length, lstm_out_dim)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 2\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88057853",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b959d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da071a6a",
   "metadata": {},
   "source": [
    "Pretty low accuracy isn't it ? Actually it is very easy to incorrectly train a deep neural net. Change the optimizer with \"adam\" instead of \"SGD\", add a dropout layer after the LSTM layer for regularization, and use early stopping :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2054a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout, early stopping, adam\n",
    "def get_lstm_model_2(vocab_size, embedding_dim, seq_length, lstm_out_dim, dropout_rate):\n",
    "    #TOFILL\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d11d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_lstm_model_2(max_vocab_size, embedding_dim, max_seq_length, lstm_out_dim, dropout_rate)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "early_stopping = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1679df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11af8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775e4ec",
   "metadata": {},
   "source": [
    "Much better. If we'd run for a longer time, we'd get a bit better results from our classic methods, but that's still quite slow for little improvement. We could also grid search for all hyper parameters (embedding and layer sizes, dropout rate, ...), but that's not the goal today, remember however that grid-search is standard when optimizing a model predictive performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d2a59",
   "metadata": {},
   "source": [
    "## Predict sentiment for arbitrary sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f39354",
   "metadata": {},
   "source": [
    "Now you can try predict the sentiment of any kind of sentence in english, try your own. You first need to encode each review as a sequence of indexes (called tokens in keras), to pad these sequances, and finally predict the score with your trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ca6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "good = \"i really liked the movie and had fun\"\n",
    "bad = \"worst movie on the planet , so boring\"\n",
    "for review in [good,bad]:\n",
    "    \n",
    "    #TOFILL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eec828",
   "metadata": {},
   "source": [
    "## Initialize embeddings with pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a747a3f0",
   "metadata": {},
   "source": [
    "The training of LSTMs is a bit heavy, one way to speed this up is to re-use pre-trained word embeddings. Many such embeddings are available on the net. Read this to understand how are produced word embeddings and why they encode information that helps with all NLP tasks:\n",
    "\n",
    "http://jalammar.github.io/illustrated-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6584d",
   "metadata": {},
   "source": [
    "We are going to use GloVe embeddings, download and load the embeddings produced from 6 billions documents from : https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec394d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../data/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc11fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {w : i for i,w in enumerate(vectorizer.get_vocabulary()) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab7aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%s unique words in vocabulary' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd5ed2",
   "metadata": {},
   "source": [
    "Given our word index, search for each of our 10000 most frequents words if they exist in the pretrained GloVe embeddings and assign them to their corresponding row index in the embedding matrix. If they don't exist in the GloVe embeddings, assign a random vector :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14049f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# Allocate the embeddings matrix\n",
    "embedding_matrix = np.zeros((max_vocab_size, embedding_dim))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6ec05",
   "metadata": {},
   "source": [
    "Now change your LSTM model so that the embedding layer is initialized with the pretrained embeddings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                   lstm_out_dim, dropout_rate, embedding_matrix):\n",
    "    #TOFILL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95827402",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_lstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af882e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce42db",
   "metadata": {},
   "source": [
    "We can see that the validation accuracy indeed progressed faster during the first epochs. For this small dataset it is not an issue, but it can save hours of training on bigger ones. It also reached a higher accuracy, which is not always the case, especially on bigger datasets.\n",
    "\n",
    "For more speed-up, at the expense of accuracy, let's fix the embeddings so that they are not trainable parameters of the model, meaning they won't be updated during training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                   lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                    trainable_embeddings):\n",
    "    #TOFILL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "trainable_embeddings = False\n",
    "\n",
    "model = get_lstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, trainable_embeddings)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a29a8",
   "metadata": {},
   "source": [
    "Notice the change in the number of trainable parameters in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3719dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3e9a2",
   "metadata": {},
   "source": [
    "By fixing the word embeddings, the training time shrunk a bit, but the validation accuracy is progressing more slowly and reaching a limit. Depending on the network architecture, the trade-off can be interesting, here not so much, just know this is a possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc236f50",
   "metadata": {},
   "source": [
    "#Â Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d2269",
   "metadata": {},
   "source": [
    "## Bidirectional and stacked LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183e3ddf",
   "metadata": {},
   "source": [
    "LSTMs parse the text from left to right, but doing it also from right to left and concatening the two output vectors improved the results. These are called bidirectional LSTMs. It is also possible to stack multiple LSTM layers.\n",
    "\n",
    "This image is a good illustration of how these two variants work:\n",
    "\n",
    "https://www.researchgate.net/figure/Illustrations-for-basic-LSTMs-and-the-three-layer-stacked-LSTM-model-for-the-sequential_fig3_313115860\n",
    "\n",
    "\n",
    "First modify your network to make a bidirectional LSTM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a735b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                     lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                    trainable_embeddings):\n",
    "    #TOFILL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_bilstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752eda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a00a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e32701f8",
   "metadata": {},
   "source": [
    "Now try stacking multiple bidirectional LSTM layers, where the number of layers `n_layers` is a parameter of the function building the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be847154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multilayer_bilstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                                lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                                trainable_embeddings, n_layers):\n",
    "    #TOFILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f84a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 100\n",
    "dropout_rate = 0.2\n",
    "n_layers = 2\n",
    "\n",
    "model = get_multilayer_bilstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, True, n_layers)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c6e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b45365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd1705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a61236e",
   "metadata": {},
   "source": [
    "As you can see, the max accuracy reached is not much better than our TF-IDF model. This happens because the full word order is actually not so important for Sentiment Analysis. For this task, Convolutional Neural Networks can attain comparable performances faster, as they have simpler architectures. But that's not true for other task such as translation, question answering, ... (which are tasks that are a bit too long to train to be included in this course, hence the choice of sentiment an analysis to practice RNNs).\n",
    "\n",
    "Let's do it with a convolutional model by using 1D convolution with a kernel size of 3 over the word embeddings (this means that it will convolve the embeddings of the consecutive words 3 by 3), followed by a 1D max pooling and a dense ReLU layer before the final sigmoid :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d40a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                                filters, dropout_rate, embedding_matrix,\n",
    "                                                trainable_embeddings):\n",
    "    #TOFILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd428666",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "filters = 100\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_conv_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       filters, dropout_rate, None, True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e63088",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175cbdd",
   "metadata": {},
   "source": [
    "# Going even further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f77c4",
   "metadata": {},
   "source": [
    "The following parts are meant to be resources to explore if you are interested in the advanced concept of attention in deep nets. There are explanation links, as well as links with code for each of them, but don't feel obliged to implement all of them, these are meant to help understanding each of these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742e66c1",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6f4e8d",
   "metadata": {},
   "source": [
    "Attention is a mechanism that changes the output of an LSTM : instead of outputting the final hidden state vector $h_n$ where $n$ is the length of the encoded text, attention plugs on top of a LSTM and returns a combination of all the hidden state vectors at each word position $\\ \\sum_{t=1}^n \\alpha_t h_t$ (where $\\alpha_t \\in (0,1))$, and thus allows to pay a different attention to each part of the text, hence the name. \n",
    "\n",
    "It has been originally proposed for sequence to sequence models, like translation models, where there is a different attention combination computed for each translated output word. It is thus less useful for text classification, but it can be adapted, by computing a single output combination of all the hidden states, as explained in Section 3.3 of the following article :\n",
    "\n",
    "https://www.aclweb.org/anthology/P16-2034.pdf\n",
    "\n",
    "Here is a link about how to apply attention for text classification with Keras:\n",
    "\n",
    "https://www.kaggle.com/yshubham/simple-lstm-for-text-classification-with-attention\n",
    "\n",
    "\n",
    "You can also read the following link to understand how attention works in sequence to sequence models, which are nothing more than a reversed LSTM (the decoder) on top of a first LSTM (the encoder), in this case for translation where it helps aligning words in two different languages :\n",
    "\n",
    "https://medium.com/towards-data-science/day-1-2-attention-seq2seq-models-65df3f49e263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7afbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc318f99",
   "metadata": {},
   "source": [
    "## Transformer architecture for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878809c2",
   "metadata": {},
   "source": [
    "State of the art models in NLP are not RNNs anymore, but Transformers. Transformers do not read text sequentially like RNNs, the core concept of Transformers is self-attention, an attention mechanism that combine separately each word embedding with the other word embeddings of the text. There are multiple such attention mechanisms called \"attention heads\" in a layer, and multiple such layers are stacked.\n",
    "\n",
    "Read this article to understand the self-attention layer:\n",
    "\n",
    "https://medium.com/towards-data-science/illustrated-self-attention-2d627e33b20a\n",
    "\n",
    "This article explains very well the Transformer for sequence to sequence models (again remember that a text classification model is just the encoder part of a sequence to sequence model) :\n",
    "\n",
    "http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "Keras code to do text classification with a Transformer :\n",
    "\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26fbc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bedeb284",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e00298",
   "metadata": {},
   "source": [
    "Current state-of-the-art performance for text classification are achieved by doing transfer learning from the BERT model. The BERT model combines different techniques including the Transformer to pretrain in an unsupervised fashion on plain text. The last layers of BERT provide a high-level contextual representation of english sentences, and can then be reused in any NLP deep model. The website HuggingFace ( https://huggingface.co/ ) hosts many pretrained deep learning models that can be reused and fine-tuned for your application.\n",
    "\n",
    "The BERT model : http://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "Reusing a pretrained BERT model from Keras-Hub for text classification : https://keras.io/keras_hub/api/models/bert/bert_text_classifier/#berttextclassifier-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210cea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9931fa2b",
   "metadata": {},
   "source": [
    "## BERT in pytorch with skorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda200e3",
   "metadata": {},
   "source": [
    "If you want to familizarize yourself with pytorch, one easy way when one is used to scikit-learn and keras is to use the `skorch` library. `skorch` implements a high level interface to pytorch with the usual `fit` and `predict` functions : https://skorch.readthedocs.io/en/stable/user/quickstart.html\n",
    "\n",
    "You can first try to reimplement a simple LSTM with pytorch. Then you can try to redo the prediction with a BERT model from huggingface : https://nbviewer.org/github/skorch-dev/skorch/blob/master/notebooks/Hugging_Face_Finetuning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e566db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41ac1d8c",
   "metadata": {},
   "source": [
    "## Querying OpenAI (ChatGPT, GPT, ...) models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a8487",
   "metadata": {},
   "source": [
    "The largest language models available nowadays like the latest versions of GPT and ChatGPT do not fit on a typical computer, but we can query them through OpenAI API to use them. However this is not free :\n",
    "https://openai.com/pricing\n",
    "\n",
    "If you feel like you want to spend some money for this, we can try two different ways. In both cases we can use the `langchain` package to interact with the openAI API through python code. \n",
    "\n",
    "The first option is to get embeddings for each text from the Ada2 model, and then build a classifier from the embeddings as features :\n",
    "https://shishirsingh66g.medium.com/langchain-applications-part-3-embedding-models-75e6a0d01545\n",
    "\n",
    "Another one is simply to ask ChatGPT whether it thinks each review of the test set is positive or negative, and compute its accuracy. Look at the `langchain` library to do so : https://medium.com/@dmitri.mahayana/chatgpt-template-using-python-langchain-c201de474122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b89c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e05341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5fd333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53123f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a526fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7753d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6186d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e73aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb41b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74eacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e543f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d4eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf9f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677dcce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a33160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c1cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560b554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37013686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757142b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c0521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32bafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4160111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c4e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534cfc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca173fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc328a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae5d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
